
library(bibliometrix)
file_article <- c("./data/Scopus_article.bib")
M_article <- convert2df(file_article, dbsource = "scopus", format = "bibtex") 
M_sub_article <- M_article[,c(1,6,11,14,17,21,25)] 
##[1] 12756     7
com_article <- missingData(M_sub_article)
com_article$allTags

M_sub_article <- M_sub_article[!is.na(M_sub_article[,2]),]                   ## 提取第一列不为空的数据
M_sub_article <- M_sub_article[complete.cases(M_sub_article[,2]),]
## [1] 12067     7
com_article <- missingData(M_sub_article)
com_article$allTags

library("quanteda")
## str(data)
## 'data.frame':    2833 obs. of  2 variables:
##  $ doc_id: chr  "text1" "text2" "text3" "text4" ...
##  $ text  : chr  "support for ukip continues to grow in the labour heartlandsiles sheldrick...."
## 

## 创建语料库
corpus <- corpus(M_sub_article,docid_field="doc_id", text_field = "AB",
               meta = list(c(M_sub_article$SN,M_sub_article$pmid,M_sub_article$TI,M_sub_article$PY)))

# Create tokens
token <- tokens(
    corpus,
    # split_hyphens = TRUE,## 是否分割"-",如果设置为TRUE,则 "self-aware" 分割为 c("self", "-", "aware")。
    # remove_numbers = TRUE,## 移除所有数字
    remove_punct = TRUE,## 删除[P]类的标点符号
    remove_symbols = TRUE,## 删除[S]类的标点符号
    remove_url = TRUE, ## 删除url链接
	remove_separators=TRUE,## 删除分隔符(remove separators and separator characters (Unicode "Separator" [Z]and "Control" [C]categories)
    split_tags =TRUE,##删除pattern_hashtag = "#\\w+#?" and pattern_username = "@[a-zA-Z0-9_]+"
	include_docvars = TRUE
  )
## 将字符转换为小写字符（lowercase text）
token <- tokens_tolower(token)
## 修剪算法（wordStem uses Martin Porter's stemming algorithm and the C libstemmer library generated by Snowball）
token <- tokens_wordstem(token)
##再次移除标点符号
token <- tokens(token, remove_punct = TRUE)
#定义终止词（define and eliminate all custom stopwords）
myStopwords <- c("can", "say","one","way","use",
                  "also","howev","tell","will",
                   "much","need","take","tend","even",
				   "like","particular","rather","said",
				   "get","well","make","ask","come","end",
				   "first","two","help","often","may",
				   "might","see","someth","thing","point",
				   "post","look","right","now","think","'ve ",
				   "'re ","anoth","put","set","new","good",
				   "want","sure","kind","larg","yes,","day","etc",
				   "quit","sinc","attempt","lack","seen","awar",
				   "littl","ever","moreov","though","found","abl",
				   "enough","far","earli","away","achiev","draw",
				   "last","never","brief","bit","entir","brief",
				   "great","lot","man","men",
				   "human","humans","study",
				   "with", "were","was","for")

token_ungd <- tokens_select(token, myStopwords, selection = "remove", case_insensitive = TRUE)

## we then create the document-feature matrix. 
## We lower and stem the words (tolower and stem) and remove common stop words (remove=stopwords()). 
##Stopwords are words that appear in texts but do not give the text a substantial meaning (e.g., “the”, “a”, or “for”)
mydfm <- dfm(token_ungd, tolower = TRUE)
## [1] 12067 52582
mydfm_remove <- dfm_select(mydfm, pattern = stopwords("english"), selection = "remove", valuetype = "fixed")
## [1] 1643 7842
mydfm_remove <- dfm_select(mydfm_remove, myStopwords, selection = "remove", valuetype = "regex")			 
##[1] 1643 7315
mydfm_remove <- dfm_select(mydfm_remove, myStopwords, selection = "remove", valuetype = "glob")			 
##[1] 1643 7315
			 
## Trim the text with dfm_trim, we filter words that appear less than 7.5% and more than 90%. 
mydfm.trim <- dfm_trim(mydfm_remove,
      min_docfreq = 0.05, # min 5.0%
	  max_docfreq = 0.95, #  max 95%
	  docfreq_type = "prop" )
##  get a look at the DFM, we now print their first 5 observations and first 10 features:
# And print the results of the first 10 observations and first 10 features in a DFM
head(dfm_sort(mydfm_remove, decreasing = TRUE, margin = "both"),n = 10, nf = 20)
head(dfm_sort(mydfm.trim, decreasing = TRUE, margin = "both"),n = 10, nf = 20)
## 1.4 分类（Classification）
##1.4.1 Determine k number of topics
library("topicmodels")
library("Rmpfr")
##A lot of the following work is based on Martin Ponweiser’s thesis, Latent Dirichlet Allocation in R. 
##  接下来的很多工作都是基于Martin Ponweiser的论文《R中的潜在狄利克雷分配》(Latent Dirichlet Allocation in r)
## The harmonic mean function:
harmonicMean <- function(logLikelihoods, precision = 2000L) {
  llMed <- median(logLikelihoods)
  as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
                                       prec = precision) + llMed))))
}
## First, an example of finding the harmonic mean for one value of k, using a burn in of 1000 and iterating 1000 times. 
## `Keep’ indicates that every keep iteration the log-likelihood is evaluated and stored. 
## The log-likelihood values are then determined by first fitting the model. 
## This returns all log-likelihood values including burn-in, i.e., these need to be omitted before calculating the harmonic mean:

k <- 15
burnin <- 1000
iter <- 1000
keep <- 50
fitted <- topicmodels::LDA(mydfm_remove, k = k, method = "Gibbs",control = list(burnin = burnin, iter = iter, keep = keep) )

## assuming that burnin is a multiple of keep
logLiks <- fitted@logLiks[-c(1:(burnin/keep))]

## This returns the harmomnic mean for k = 25 topics.
harmonicMean(logLiks)
## [1] -9320071



##To find the best value for k for our corpus, we do this over a sequence of topic models with different vales for k. 
## This will generate numerous topic models with different numbers of topics, creating a vector to hold the k values. 
seqk <- seq(2, 100, 5)
burnin <- 1000
iter <- 1000
keep <- 50
system.time(fitted_many <- lapply(seqk, function(k) topicmodels::LDA(mydfm_remove, k = k,
            method = "Gibbs",control = list(burnin = burnin,  iter = iter, keep = keep) )))

# extract logliks from each topic
logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])

# compute harmonic means
hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))
library("ggplot2")
ldaplot <- ggplot(data.frame(seqk, hm_many), aes(x=seqk, y=hm_many)) + geom_path(lwd=1.5) +
  theme(text = element_text(family= NULL),
        axis.title.y=element_text(vjust=1, size=16),
        axis.title.x=element_text(vjust=-.5, size=16),
        axis.text=element_text(size=16),
        plot.title=element_text(size=20)) +
  xlab('Number of Topics') +
  ylab('Harmonic Mean') +
     annotate("text", x = 25, y = -80000, label = paste("The optimal number of topics is", seqk[which.max(hm_many)])) +
  ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of NEN LLIS", atop(italic("How many distinct topics in the abstracts?"), ""))))
ldaplot

seqk[which.max(hm_many)]
## [1] 97



## 1.4.2 潜变量模型（Latent Dirichlet Allocation）
# transform variables
# Load the package
library(topicmodels)
library(quanteda.textmodels)
set.seed(123)
# Assign an arbitrary number of topics
topic.count <- 97

# Convert the trimmed DFM to a topicmodels object
dfm2topicmodels <- convert(mydfm_remove, to = "topicmodels")
lda.model <- LDA(dfm2topicmodels, topic.count)
as.data.frame(terms(lda.model, 6))
lda.similarity <- as.data.frame(lda.model@beta) %>%
  scale() %>%
library(factoextra)
library(NbClust)
library("data.table")
NbClust(data = lda.similarity, diss = NULL, distance = "euclidean",
        min.nc = 2, max.nc = 15, method = "ward.D2")
# Elbow method
lda.similarity <- fviz_nbclust(data, kmeans,k.max=20, method = "wss") +
    geom_vline(xintercept = 15, linetype = 2)+
  labs(subtitle = "Elbow method")# Silhouette method

pdf('Optimal_number_topics.pdf',width = 6,height = 6)
ldaplot
lda.similarity
dev.off()	 
# Assign an arbitrary number of topics
topic.count <- 15

# Convert the trimmed DFM to a topicmodels object
dfm2topicmodels <- convert(mydfm_remove, to = "topicmodels")
lda.model <- LDA(dfm2topicmodels, topic.count)
as.data.frame(terms(lda.model, 6))	 
	 
	 
## 展示词云
LDA_model <- lda.model
tmResult <- posterior(LDA_model)
library(ggwordcloud)
library(see)
# visualize topics as word cloud
topicToViz <- 15 # change for your own topic of interest
# topicToViz <- grep('mexico', topicNames)[1] # Or select a topic by a term contained in its name

# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)

# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
p_topic_15   <-  ggplot(data= data.frame(words, probabilities))+
      geom_text_wordcloud(aes(label=words,size=probabilities))+
	  scale_radius(range = c(0, 30), limits = c(0, NA)) +
      theme_minimal()

	 
## 1.4.3 Structural topic models（structural topic models (STM) are a popular extension of the standard LDA models.）
topic.count <- 15 # Assigns the number of topics
# Calculate the STM 
dfm2stm <- convert(mydfm_remove, to = "stm")
model.stm <- stm(
  dfm2stm$documents,
  dfm2stm$vocab,
  K = topic.count,
  data = dfm2stm$meta,
  init.type = "Spectral"
)
as.data.frame(t(labelTopics(model.stm, n = 10)$prob))
## The following plot allows us to intuitively get information on the share of the different topics at the overall corpus.
plot(
  model.stm,
  type = "summary",
  text.cex = 0.5,
  main = "STM topic shares",
  xlab = "Share estimation"
)
stm::cloud(model.stm,
           topic = 4,
           scale = c(2.25, .5))
plot(model.stm,
     type = "perspectives",
     topics = c(4, 5),
     main = "Putting two different topics in perspective")



## LDAvis is a nice interactive visualization of LDA results
LDA_model <- lda.model
tmResult <- posterior(LDA_model)
LDA_dtm <- mydfm_remove
ncol(LDA_dtm)
## [1] 7315
nrow(LDA_dtm)
##  [1] 1643
#主题是整个词汇表的概率分布（topics are probability distribtions over the entire vocabulary）
beta <- tmResult$terms   # get beta from results
dim(beta) 
## [1]   25 7315
# for every document we have a probability distribution of its contained topics
theta <- tmResult$topics 
dim(theta)   
exampleIds <- c(2, 100, 200)
cat(corpus[exampleIds[1]])
cat(corpus[exampleIds[2]])
cat(corpus[exampleIds[3]])

# re-rank top topic terms for topic names
topicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = " ")

# What are the most probable topics in the entire collection?
topicProportions <- colSums(theta) / nrow(LDA_dtm)  # mean probablities over all paragraphs
names(topicProportions) <- topicNames     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order
K <- 15
countsOfPrimaryTopics <- rep(0, K)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nrow(LDA_dtm)) {
  topicsPerDoc <- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
sort(countsOfPrimaryTopics, decreasing = TRUE)
textdata <- M_sub_article 
# get mean topic proportions per decade
topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$PY), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames

# reshape data frame
library("reshape2")
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "decade")

# plot topic proportions per deacde as bar plot
require(pals)
topic_proportions <- ggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(25), "FF"), name = "decade") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
# LDAvis browser
library(LDAvis)
library("tsne")
svd_tsne <- function(x) tsne(svd(x)$u)
json <- createJSON(
  phi = tmResult$terms, 
  theta = tmResult$topics, 
  doc.length = rowSums(LDA_dtm), 
  vocab = colnames(LDA_dtm), 
  term.frequency = colSums(LDA_dtm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="", ylab="")
)
serVis(json)
library("cowplot")
p_topic_combine <- plot_grid(p_topic_1, p_topic_2, 
                p_topic_3, p_topic_4,p_topic_5, 
				p_topic_6,p_topic_7, p_topic_8,
				p_topic_9, p_topic_10,p_topic_11, 
				p_topic_12,p_topic_13, p_topic_14,
				p_topic_15,labels = c('C','D','E','F','G',
				                      'H','I','J','K','L',
									  'M','N','O','P','Q'), 
				label_size = 5, ncol = 4)

pdf('result11.pdf',width = 40,height = 25)
p_topic_combine
dev.off()
pdf('topic_proportions.pdf',width = 20,height = 10)
topic_proportions
dev.off()


