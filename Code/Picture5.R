library(bibliometrix)
# Load all required packages
library(tidyverse)        # Also loads dplyr, ggplot2, and haven
library(quanteda)         # For NLP
library(readtext)         # To read .txt files
library(stm)              # For structural topic models
library(stminsights)      # For visual exploration of STM
library(wordcloud)        # To generate wordclouds
library(gsl)              # Required for the topicmodels package
library(topicmodels)      # For topicmodels
library(caret)            # For machine learning
file <- c("./Data/scopus_article.bib")
M <- convert2df(file, dbsource = "scopus", format = "bibtex") 
M_sub <- M[,c(6,11,17,21,25)]
## [1] 12521     5
M_sub <- M_sub[!is.na(M_sub[,1]),]                   
M_sub <- M_sub[complete.cases(M_sub[,1]),]
## [1] 11920     5
M_sub <- data.frame(M_sub)
M_sub$doc_id <- paste0("text",1:nrow(M_sub),sep="")
library("quanteda")
## Createcorpus
corpus <- corpus(M_sub,docid_field="doc_id", text_field = "AB",
               meta = list(c(M_sub$SN,M_sub$pmid,M_sub$TI,M_sub$PY)))

# Create tokens
token <- tokens(
    corpus,
    remove_numbers = TRUE,
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_url = TRUE, 
	remove_separators=TRUE,
    split_tags =TRUE,
	include_docvars = TRUE
  )
## Lowercase text
token <- tokens_tolower(token)
## wordStem uses Martin Porter's stemming algorithm and the C libstemmer library generated by Snowball
token <- tokens_wordstem(token)
##Remove punctuation again
token <- tokens(token, remove_punct = TRUE)


## Define and eliminate all custom stopwords
myStopwords <- c(stopwords("english"),
 "people", "time", "day", "years", "just", "really", "one", "can", "like", "go", "get", "gt",
 "amp", "now", "one","two","h", "p", "ci","per","der","und","r","kg","ii","get", "much", "many", "every", "lot", "even", "also")

token_DEstop <- tokens_select(token, myStopwords, selection = "remove", case_insensitive = TRUE)








## we then create the document-feature matrix. 
## We lower and stem the words (tolower and stem) and remove common stop words (remove=stopwords()). 
##Stopwords are words that appear in texts but do not give the text a substantial meaning (e.g., “the”, “a”, or “for”)
mydfm <- dfm(token_DEstop, tolower = TRUE)
##[1] 12067 51729

topfeatures(mydfm,n=100)
## sperm        patient       infertil            men          studi 
# 26748          22545          15083          14416          13518 
##  male          group         result       signific     testicular 
# 12865          12735          10202          10037           9576 
mydfm_remove <- dfm_select(mydfm, pattern = stopwords("english"), selection = "remove", valuetype = "fixed")
##[1] [1] 12067 61970
 head(textstat_frequency(mydfm_remove),n=20)
##          feature frequency rank docfreq group
#1           sperm     26748    1    6882   all
#2         patient     22545    2    6916   all
#3        infertil     15083    3    5875   all
#4             men     14416    4    5049   all
#5           studi     13518    5    7146   all
#6            male     12865    6    6044   all
#7           group     12735    7    3874   all
#8          result     10202    8    6978   all
#9        signific     10037    9    5190   all
#10     testicular      9576   10    3647   all
#11            use      9406   11    5392   all
#12           cell      9031   12    3056   all
#13    azoospermia      8957   13    5128   all
#14         fertil      8934   14    4416   all
#15          semen      8471   15    3550   all
#16              p      8325   16    2985   all
#17          level      8202   17    3500   all
#18         normal      7159   18    4009   all
#19    spermatozoa      7020   19    2641   all
#20           gene      6798   20    2434   all			 
## Trim the text with dfm_trim, we filter words that appear less than 1% and more than 95%. 
mydfm.trim <- dfm_trim(mydfm_remove,
      min_docfreq = 0.01, # min 1.0%
	  max_docfreq = 0.99, #  max 99.0%
	  docfreq_type = "prop" )


##1 Classification
##1.1 Determine k number of topics
library("topicmodels")
library("Rmpfr")
library("ldatuning")
#
ldatuning.metrics <- FindTopicsNumber(mydfm.trim, 
       topics = seq(from = 2, to = 15, by = 1), 
	   metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), 
	   method = "Gibbs", 
	   control = list(seed = 77), mc.cores = 2L, verbose = TRUE
)
 FindTopicsNumber_plot(ldatuning.metrics)

k <- 15
burnin <- 1000
iter <- 1000
keep <- 50
LDA_model <- topicmodels::LDA(mydfm.trim, k = k, method = "Gibbs",control = list(burnin = burnin, iter = iter, keep = keep) )
tmResult <- posterior(LDA_model)


library("ggwordcloud")

plotlist = list()
for (n in 1:k) {
    topicToViz <- n
	top30terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:30]
	words <- names(top30terms)
	probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:30]
    # visualize the terms as wordcloud
    p_topic_n   <-  ggplot(data= data.frame(words, probabilities))+
      geom_text_wordcloud(aes(label=words,size=probabilities))+
	  ## scale_radius(range = c(0, 30), limits = c(0, NA)) +
	  scale_radius(range = c(2.5, 30), limits = c(0, NA)) +
	  theme_minimal()
	plotlist[[n]] = p_topic_n  
	}
	 
library(gridExtra)
p_topic <- marrangeGrob(grobs=plotlist, nrow=4, ncol = 4)

#2 Topics are probability distribtions over the entire vocabulary
beta <- tmResult$terms   # get beta from results
dim(beta) 
##[1]   15 1543
# for every document we have a probability distribution of its contained topics
theta <- tmResult$topics 
dim(theta)   
exampleIds <- c(2, 100, 200)
cat(corpus[exampleIds[1]])
cat(corpus[exampleIds[2]])
cat(corpus[exampleIds[3]])

# re-rank top topic terms for topic names
topicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = " ")

# What are the most probable topics in the entire collection?
topicProportions <- colSums(theta) / nrow(mydfm.trim)  # mean probablities over all paragraphs
names(topicProportions) <- topicNames     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order
countsOfPrimaryTopics <- rep(0, k)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nrow(mydfm.trim)) {
  topicsPerDoc <- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
sort(countsOfPrimaryTopics, decreasing = TRUE)  
# get mean topic proportions per decade
topic_proportion_per_year <- aggregate(theta, by = list(year = M_sub$PY), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_year)[2:(k+1)] <- topicNames


# reshape data frame
library("reshape2")
vizDataFrame <- melt(topic_proportion_per_year, id.vars = "year")
# plot topic proportions per year as bar plot
require(pals)
topic_year <- ggplot(vizDataFrame, aes(x=year, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(25), "FF"), name = "decade") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

pdf('result12.pdf',width = 40,height = 25)
FindTopicsNumber_plot(ldatuning.metrics)
p_topic
topic_year
dev.off() 	 
